# Databricks notebook source
# MAGIC %md
# MAGIC # Writing Data Quality Rules with DQX
# MAGIC
# MAGIC This notebook demonstrates how to create a set of data quality rules for DQX.
# MAGIC
# MAGIC DQX offers many built-in functions to validate common data quality checks (e.g. for nullability and uniqueness). 
# MAGIC
# MAGIC **Rules** define the types of validations performed on your data. These rules can be:
# MAGIC - Inferred from existing data using DQX's `DQProfiler` and `DQGenerator`
# MAGIC - Defined in natural language or YAML
# MAGIC - Stored and loaded using configuration files or tables

# COMMAND ----------

# MAGIC %pip install databricks-labs-dqx[llm]
# MAGIC %restart_python

# COMMAND ----------

# MAGIC %md
# MAGIC ## Creating Example Data
# MAGIC We'll create an example invoices dataset to work with throughout this notebook.
# MAGIC
# MAGIC Our data will contain some typical data quality issues. We'll use DQX to author and apply rules to report these issues.

# COMMAND ----------

# Create some sample invoices data:
invoices_data = [
  [1, 110100, "2025-01-01", "user_10992@email.com", 10, 100.0, 1000.0],
  [2, 110102, "2025-01-01", "user_10083@email.com", 5, 50.0, 250.0],
  [3, None, "2025-01-01", "user_11017@email.com", 1, 100.0, 100.0],
  [4, 110104, "2025-01-01", None, 3, 300.0, 900.0],
  [5, 110108, "2025-01-01", "user_12491@email", -1, 40.0, 1000.0],
]
invoices_schema = (
  "invoice_id int, invoice_number long, invoice_date string, customer_email string, "
  "quantity int, unit_price double, total_price double"
)
invoices_df = spark.createDataFrame(
  invoices_data,
  schema=invoices_schema
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Using the `DQProfiler`
# MAGIC `DQProfiler` profiles datasets to generate **candidate rules** from existing data. This can be useful as a one-time activity to get started with rule definition.
# MAGIC
# MAGIC Data engineers and data stewards should review and refine candidate rules generated by the `DQProfiler` and `DQGenerator`.

# COMMAND ----------

from databricks.labs.dqx.profiler.profiler import DQProfiler
from databricks.sdk import WorkspaceClient
from pprint import pprint

# Create the WorkspaceClient:
ws = WorkspaceClient()

# Create the DQProfiler:
profiler = DQProfiler(ws)

# Profile the invoices dataset:
options = {"sample_fraction": 1.0, "limit": 100}
summary_stats, profiles = profiler.profile(invoices_df)
pprint(profiles[0])

# COMMAND ----------

from databricks.labs.dqx.profiler.generator import DQGenerator

# Create the DQGenerator:
generator = DQGenerator(ws)

# Generate rules from the DQProfiles:
rules = generator.generate_dq_rules(profiles)
pprint(rules[0])

# COMMAND ----------

import yaml

rules_config = yaml.safe_dump(rules)
print(rules_config)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Storing Rules in a Table
# MAGIC Rules can be stored and loaded from a table in Unity Catalog. Access can be granted to data stewards to refine and update profiled rules. Use the `DQEngine` to store checks.

# COMMAND ----------

from databricks.labs.dqx.engine import DQEngine
from databricks.labs.dqx.config import TableChecksStorageConfig

# Create the DQEngine:
engine = DQEngine(ws)

# Create the TableChecksStorageConfig:
table_checks_storage_config = TableChecksStorageConfig(
  location="ghansen.ops.data_quality_rules",
  run_config_name="invoices_checks",
  mode="overwrite"
)

# Save the checks to a table:
engine.save_checks(rules, table_checks_storage_config)

# Load and display the checks:
display(spark.table("ghansen.ops.data_quality_rules"))

# COMMAND ----------

# Remove the `invoice_id_isnt_in_range` and `quantity_isnt_in_range` rules:
spark.sql(
  """
  DELETE FROM
    ghansen.ops.data_quality_rules
  WHERE
    name IN ("invoice_id_isnt_in_range", "quantity_isnt_in_range")
  """
)

display(spark.table("ghansen.ops.data_quality_rules"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Defining Rules Manually
# MAGIC Rules can also be defined manually using either **natural language** or **configuration**.

# COMMAND ----------

# Create prompts for new rules:
prompt = """
  customer email should end with ".com"
  total price should be positive
  unit price should be positive
"""

# Generate new rules:
ai_generated_rules = generator.generate_dq_rules_ai_assisted(prompt)
pprint(ai_generated_rules)

# COMMAND ----------

# Save the AI-generated rules to our rules table:
table_checks_storage_config.mode = "append"
engine.save_checks(ai_generated_rules, table_checks_storage_config)
display(spark.table("ghansen.ops.data_quality_rules"))

# COMMAND ----------

# MAGIC %md
# MAGIC # Loading and Applying Rules
# MAGIC We can now load and apply rules from our rules table. We'll use the `DQEngine` to apply rules to our sample data.

# COMMAND ----------

# Load the rules from our table:
all_rules = engine.load_checks(table_checks_storage_config)
pprint(all_rules)

# Validate the rules:
rule_validation_status = engine.validate_checks(all_rules)
pprint(rule_validation_status)

# COMMAND ----------

# Apply rules to our sample data:
checked_invoices_df = engine.apply_checks_by_metadata(invoices_df, all_rules)
display(checked_invoices_df)